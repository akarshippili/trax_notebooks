{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "trax_named_entity_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "coursera": {
      "schema_names": [
        "NLPC3-3A"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akarshippili/trax_notebooks/blob/main/trax_named_entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEY_jlQQR9SP",
        "scrolled": true
      },
      "source": [
        "!pip install -q -U trax\n",
        "\n",
        "import trax \n",
        "from trax import layers as tl\n",
        "import os \n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wiw0vie4pgZs"
      },
      "source": [
        "from utils import get_params, get_vocab\n",
        "import random as rnd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jur1JnXnCtr",
        "scrolled": true,
        "outputId": "2344332c-711e-42c2-8719-a22039c5e51b"
      },
      "source": [
        "# display original kaggle data\n",
        "data = pd.read_csv(\"ner_dataset.csv\", encoding = \"ISO-8859-1\") \n",
        "train_sents = open('data/small/train/sentences.txt', 'r').readline()\n",
        "train_labels = open('data/small/train/labels.txt', 'r').readline()\n",
        "print('SENTENCE:', train_sents)\n",
        "print('SENTENCE LABEL:', train_labels)\n",
        "print('ORIGINAL DATA:\\n', data.head(5))\n",
        "del(data, train_sents, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
            "\n",
            "SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
            "\n",
            "ORIGINAL DATA:\n",
            "     Sentence #           Word  POS Tag\n",
            "0  Sentence: 1      Thousands  NNS   O\n",
            "1          NaN             of   IN   O\n",
            "2          NaN  demonstrators  NNS   O\n",
            "3          NaN           have  VBP   O\n",
            "4          NaN        marched  VBN   O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoH6yBWVfzTb"
      },
      "source": [
        "<a name=\"1.1\"></a>\n",
        "## 1.1  Importing the Data\n",
        "\n",
        "In this part, we will import the preprocessed data and explore it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UauHjIKHWC0N",
        "scrolled": true
      },
      "source": [
        "vocab, tag_map = get_vocab('data/large/words.txt', 'data/large/tags.txt')\n",
        "t_sentences, t_labels, t_size = get_params(vocab, tag_map, 'data/large/train/sentences.txt', 'data/large/train/labels.txt')\n",
        "v_sentences, v_labels, v_size = get_params(vocab, tag_map, 'data/large/val/sentences.txt', 'data/large/val/labels.txt')\n",
        "test_sentences, test_labels, test_size = get_params(vocab, tag_map, 'data/large/test/sentences.txt', 'data/large/test/labels.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm2P8y7zNgdU",
        "scrolled": true,
        "outputId": "bb6d0db4-14cf-4ab8-90a0-cb89efa12d1c"
      },
      "source": [
        "# vocab translates from a word to a unique number\n",
        "print('vocab[\"the\"]:', vocab[\"the\"])\n",
        "# Pad token\n",
        "print('padded token:', vocab['<PAD>'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab[\"the\"]: 9\n",
            "padded token: 35180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ZzMamaPcQXWP",
        "scrolled": true,
        "outputId": "4f04364c-a88c-4e77-f8bb-9bfcb422d5e9"
      },
      "source": [
        "print(tag_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'B-art': 8, 'I-art': 9, 'I-per': 10, 'I-gpe': 11, 'I-tim': 12, 'B-nat': 13, 'B-eve': 14, 'I-eve': 15, 'I-nat': 16}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM9B_Rwxd01i",
        "scrolled": true,
        "outputId": "ae1e9124-524b-429e-ba19-0c8670cfa055"
      },
      "source": [
        "print('The number of outputs is tag_map', len(tag_map))\n",
        "\n",
        "g_vocab_size = len(vocab)\n",
        "print(f\"Num of vocabulary words: {g_vocab_size}\")\n",
        "print('The vocab size is', len(vocab))\n",
        "print('The training size is', t_size)\n",
        "print('The validation size is', v_size)\n",
        "print('An example of the first sentence is', t_sentences[0])\n",
        "print('An example of its corresponding label is', t_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of outputs is tag_map 17\n",
            "Num of vocabulary words: 35181\n",
            "The vocab size is 35181\n",
            "The training size is 33570\n",
            "The validation size is 7194\n",
            "An example of the first sentence is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]\n",
            "An example of its corresponding label is [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP7zQC8knCt_",
        "scrolled": true
      },
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: data_generator\n",
        "def data_generator(batch_size, x, y, pad, shuffle=False, verbose=False):\n",
        "    num_lines = len(x)\n",
        "    lines_index = [*range(num_lines)]\n",
        "\n",
        "    if shuffle:\n",
        "        rnd.shuffle(lines_index)\n",
        "    \n",
        "    index = 0 # tracks current location in x, y\n",
        "    while True:\n",
        "        buffer_x = [0] * batch_size # Temporal array to store the raw x data for this batch\n",
        "        buffer_y = [0] * batch_size # Temporal array to store the raw y data for this batch\n",
        "                        \n",
        "        max_len = 0\n",
        "        for i in range(batch_size):\n",
        "            if index >= num_lines:\n",
        "                index = 0\n",
        "                if shuffle:\n",
        "                    rnd.shuffle(lines_index)\n",
        "            \n",
        "            buffer_x[i] = x[lines_index[index]]            \n",
        "            buffer_y[i] = y[lines_index[index]]\n",
        "            lenx = len(x[lines_index[index]])    #length of current x[]\n",
        "            if lenx > max_len:\n",
        "                max_len = lenx                   #max_len tracks longest x[]\n",
        "            \n",
        "            index += 1\n",
        "\n",
        "        # create X,Y, NumPy arrays of size (batch_size, max_len) 'full' of pad value\n",
        "        X = np.full((batch_size, max_len), pad)\n",
        "        Y = np.full((batch_size, max_len), pad)\n",
        "\n",
        "        # copy values from lists to NumPy arrays. Use the buffered values\n",
        "        for i in range(batch_size):\n",
        "            x_i = buffer_x[i]\n",
        "            y_i = buffer_y[i]            \n",
        "            for j in range(len(x_i)):\n",
        "                X[i, j] = x_i[j]\n",
        "                Y[i, j] = y_i[j]\n",
        "\n",
        "        if verbose: print(\"index=\", index)\n",
        "        yield((X,Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3fwE3PMhOW4",
        "scrolled": true,
        "outputId": "d86f6667-3495-4d6b-d2d9-81e34a62a7f4"
      },
      "source": [
        "batch_size = 5\n",
        "mini_sentences = t_sentences[0: 8]\n",
        "mini_labels = t_labels[0: 8]\n",
        "dg = data_generator(batch_size, mini_sentences, mini_labels, vocab[\"<PAD>\"], shuffle=False, verbose=True)\n",
        "X1, Y1 = next(dg)\n",
        "X2, Y2 = next(dg)\n",
        "print(Y1.shape, X1.shape, Y2.shape, X2.shape)\n",
        "print(X1[0][:], \"\\n\", Y1[0][:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index= 5\n",
            "index= 2\n",
            "(5, 30) (5, 30) (5, 30) (5, 30)\n",
            "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
            "    12    13    14     9    15     1    16    17    18    19    20    21\n",
            " 35180 35180 35180 35180 35180 35180] \n",
            " [    0     0     0     0     0     0     1     0     0     0     0     0\n",
            "     1     0     0     0     0     0     2     0     0     0     0     0\n",
            " 35180 35180 35180 35180 35180 35180]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SWxKhkVLr3P"
      },
      "source": [
        "<a name=\"2\"></a>\n",
        "# Part 2:  Building the model\n",
        "\n",
        "You will now implement the model. You will be using Google's TensorFlow. Your model will be able to distinguish the following:\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "<img src = 'ner1.png' width=\"width\" height=\"height\" style=\"width:500px;height:150px;\"/>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "The model architecture will be as follows: \n",
        "\n",
        "<img src = 'ner2.png' width=\"width\" height=\"height\" style=\"width:600px;height:250px;\"/>\n",
        "\n",
        "Concretely: \n",
        "\n",
        "* Use the input tensors you built in your data generator\n",
        "* Feed it into an Embedding layer, to produce more semantic entries\n",
        "* Feed it into an LSTM layer\n",
        "* Run the output through a linear layer\n",
        "* Run the result through a log softmax layer to get the predicted class for each word.\n",
        "\n",
        "Good news! We won't make you implement the LSTM unit drawn above. However, we will ask you to build the model. \n",
        "\n",
        "<a name=\"ex02\"></a>\n",
        "### Exercise 02\n",
        "\n",
        "**Instructions:** Implement the initialization step and the forward function of your Named Entity Recognition system.  \n",
        "Please utilize help function e.g. `help(tl.Dense)` for more information on a layer\n",
        "   \n",
        "- [tl.Serial](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py#L26): Combinator that applies layers serially (by function composition).\n",
        "    - You can pass in the layers as arguments to `Serial`, separated by commas. \n",
        "    - For example: `tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))` \n",
        "\n",
        "\n",
        "-  [tl.Embedding](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113): Initializes the embedding. In this case it is the dimension of the model by the size of the vocabulary. \n",
        "    - `tl.Embedding(vocab_size, d_feature)`.\n",
        "    - `vocab_size` is the number of unique words in the given vocabulary.\n",
        "    - `d_feature` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).\n",
        "    \n",
        "\n",
        "-  [tl.LSTM](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py#L87):`Trax` LSTM layer of size d_model. \n",
        "    - `LSTM(n_units)` Builds an LSTM layer of n_cells.\n",
        "\n",
        "\n",
        "\n",
        "-  [tl.Dense](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L28):  A dense layer.\n",
        "    - `tl.Dense(n_units)`: The parameter `n_units` is the number of units chosen for this dense layer.  \n",
        "\n",
        "\n",
        "- [tl.LogSoftmax](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242): Log of the output probabilities.\n",
        "    - Here, you don't need to set any parameters for `LogSoftMax()`.\n",
        " \n",
        "\n",
        "**Online documentation**\n",
        "\n",
        "- [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators)\n",
        "\n",
        "- [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)\n",
        "\n",
        "-  [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM)\n",
        "\n",
        "-  [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)\n",
        "\n",
        "- [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax)    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5TfzRIntLhH"
      },
      "source": [
        "def NER2(vocab_size = len(vocab),d_model = 50,tags = tag_map):\n",
        "  return tl.Serial(\n",
        "      tl.Embedding(vocab_size,d_model),\n",
        "      tl.LSTM(d_model),\n",
        "      tl.Dense(len(tags)),\n",
        "      tl.LogSoftmax()\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL5u72u8Lr3Q",
        "scrolled": true
      },
      "source": [
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: NER\n",
        "def NER(vocab_size=35181, d_model=50, tags=tag_map):\n",
        "    model = tl.Serial(\n",
        "      tl.Embedding(vocab_size, d_model), # Embedding layer\n",
        "      tl.LSTM(d_model), # LSTM layer\n",
        "      tl.Dense(len(tags)), # Dense layer with len(tags) units\n",
        "      tl.LogSoftmax()  # LogSoftmax layer\n",
        "      )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrGdYpPvLr3U",
        "scrolled": true,
        "outputId": "ee3fdb2a-b600-49ef-afef-2a0a3740556d"
      },
      "source": [
        "model = NER()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial[\n",
            "  Embedding_35181_50\n",
            "  LSTM_50\n",
            "  Dense_17\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_t5tFu8uVQ2",
        "outputId": "5de08f69-1472-4513-f598-d4d1eb3ee1ea"
      },
      "source": [
        "modelx= NER2()\n",
        "print(modelx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial[\n",
            "  Embedding_35181_50\n",
            "  LSTM_50\n",
            "  Dense_17\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPBR1YrRmEAH",
        "scrolled": true
      },
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "rnd.seed(33)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_generator = trax.data.inputs.add_loss_weights(\n",
        "    data_generator(batch_size, t_sentences, t_labels, vocab['<PAD>'], True),\n",
        "    id_to_mask=vocab['<PAD>'])\n",
        "\n",
        "eval_generator = trax.data.inputs.add_loss_weights(\n",
        "    data_generator(batch_size, v_sentences, v_labels, vocab['<PAD>'], True),\n",
        "    id_to_mask=vocab['<PAD>'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV27PerULr3a",
        "scrolled": true
      },
      "source": [
        "\n",
        "def train_model(NER, train_generator, eval_generator, train_steps=1, output_dir='model'):\n",
        "\n",
        "    train_task = training.TrainTask(train_generator,loss_layer = tl.CrossEntropyLoss(), optimizer = trax.optimizers.Adam(0.01),)\n",
        "\n",
        "    eval_task = training.EvalTask(labeled_data = eval_generator,metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], n_eval_batches = 10)\n",
        "\n",
        "    training_loop = training.Loop(NER, train_task, eval_tasks = [eval_task], output_dir = output_dir) # The output directory\n",
        "\n",
        "    # Train with train_steps\n",
        "    training_loop.run(n_steps = train_steps)\n",
        "    ### END CODE HERE ###\n",
        "    return training_loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tIc4nuonCue"
      },
      "source": [
        "On your local machine, you can run this training for 1000 train_steps and get your own model. This training takes about 5 to 10 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU-j8hs-nCue",
        "outputId": "02f6d412-637e-4383-b3e4-e76a08007278"
      },
      "source": [
        "train_steps = 100            # In coursera we can only train 100 steps\n",
        "!rm -f 'model/model.pkl.gz'  # Remove old model.pkl if it exists\n",
        "\n",
        "# Train the model\n",
        "training_loop = train_model(model, train_generator, eval_generator, train_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:372: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_id has been renamed to jax.process_index. This alias \"\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:385: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 1780117\n",
            "Step      1: Ran 1 train steps in 2.95 secs\n",
            "Step      1: train CrossEntropyLoss |  0.13955270\n",
            "Step      1: eval  CrossEntropyLoss |  0.16062951\n",
            "Step      1: eval          Accuracy |  0.95319662\n",
            "\n",
            "Step    100: Ran 99 train steps in 63.75 secs\n",
            "Step    100: train CrossEntropyLoss |  0.15146372\n",
            "Step    100: eval  CrossEntropyLoss |  0.14837703\n",
            "Step    100: eval          Accuracy |  0.95700682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4r-gXOZLr3j"
      },
      "source": [
        "<a name=\"4\"></a>\n",
        "# Part 4:  Compute Accuracy\n",
        "\n",
        "You will now evaluate in the test set. Previously, you have seen the accuracy on the training set and the validation (noted as eval) set. You will now evaluate on your test set. To get a good evaluation, you will need to create a mask to avoid counting the padding tokens when computing the accuracy. \n",
        "\n",
        "<a name=\"ex04\"></a>\n",
        "### Exercise 04\n",
        "\n",
        "**Instructions:** Write a program that takes in your model and uses it to evaluate on the test set. You should be able to get an accuracy of 95%.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmIvd_GXnCuk"
      },
      "source": [
        "\n",
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>More Detailed Instructions </b></font>\n",
        "</summary>\n",
        "\n",
        "* *Step 1*: model(sentences) will give you the predicted output. \n",
        "\n",
        "* *Step 2*: Prediction will produce an output with an added dimension. For each sentence, for each word, there will be a vector of probabilities for each tag type. For each sentence,word, you need to pick the maximum valued tag. This will require `np.argmax` and careful use of the `axis` argument.\n",
        "* *Step 3*: Create a mask to prevent counting pad characters. It has the same dimension as output. An example below on matrix comparison provides a hint.\n",
        "* *Step 4*: Compute the accuracy metric by comparing your outputs against your test labels. Take the sum of that and divide by the total number of **unpadded** tokens. Use your mask value to mask the padded tokens. Return the accuracy. \n",
        "</detail>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaSy_NywnCul",
        "scrolled": true,
        "outputId": "526753a2-7def-4e01-b39e-d0dba85039fb"
      },
      "source": [
        "#Example of a comparision on a matrix \n",
        "a = np.array([1, 2, 3, 4])\n",
        "a == 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False,  True, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H0Kx1rnnCun",
        "scrolled": true,
        "outputId": "07accada-6c6a-456f-c65c-770aa5550b89"
      },
      "source": [
        "# create the evaluation inputs\n",
        "x, y = next(data_generator(len(test_sentences), test_sentences, test_labels, vocab['<PAD>']))\n",
        "print(\"input shapes\", x.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shapes (7194, 70) (7194, 70)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdmIXVoGxBEi",
        "outputId": "73e3189b-a35f-456b-dd7b-70b369e48f61"
      },
      "source": [
        "type(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh16zSTonCuq",
        "outputId": "e19ff6a1-8680-4897-ad6e-fbcc344e4016"
      },
      "source": [
        "# sample prediction\n",
        "tmp_pred = model(x)\n",
        "print(type(tmp_pred))\n",
        "print(f\"tmp_pred has shape: {tmp_pred.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'jaxlib.xla_extension.DeviceArray'>\n",
            "tmp_pred has shape: (7194, 70, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78l5MTSBnCut"
      },
      "source": [
        "Note that the model's prediction has 3 axes: \n",
        "- the number of examples\n",
        "- the number of words in each example (padded to be as long as the longest sentence in the batch)\n",
        "- the number of possible targets (the 17 named entity tags)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ek59ro9nCut"
      },
      "source": [
        "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: evaluate_prediction\n",
        "def evaluate_prediction(pred, labels, pad):\n",
        "    outputs = np.argmax(pred, axis=2)\n",
        "    print(\"outputs shape:\", outputs.shape)\n",
        "\n",
        "    mask = labels != pad\n",
        "    print(\"mask shape:\", mask.shape, \"mask[0][20:30]:\", mask[0][20:30])\n",
        "    accuracy = np.sum(outputs == labels) / float(np.sum(mask))\n",
        "    return accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCWFwt3m1sgL",
        "scrolled": true,
        "outputId": "f4fe7f19-8b85-4493-c410-ebba2af5212f"
      },
      "source": [
        "accuracy = evaluate_prediction(model(x), y, vocab['<PAD>'])\n",
        "print(\"accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "outputs shape: (7194, 70)\n",
            "mask shape: (7194, 70) mask[0][20:30]: [ True  True  True False False False False False False False]\n",
            "accuracy:  0.95593786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym_ENAbfzTQQ",
        "outputId": "173d7fea-8ea4-4c3f-e197-fa1022b75d69"
      },
      "source": [
        "accuracy = evaluate_prediction(tmp_pred, y, vocab['<PAD>'])\n",
        "print(\"accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "outputs shape: (7194, 70)\n",
            "mask shape: (7194, 70) mask[0][20:30]: [ True  True  True False False False False False False False]\n",
            "accuracy:  0.95593786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2FEleAFLr3r"
      },
      "source": [
        "<a name=\"5\"></a>\n",
        "# Part 5:  Testing with your own sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOeTPAx_Lr3t"
      },
      "source": [
        "Below, you can test it out with your own sentence! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K4SyB20cHRf",
        "scrolled": true
      },
      "source": [
        "# This is the function you will be using to test your own sentence.\n",
        "def predict(sentence, model, vocab, tag_map):\n",
        "    s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
        "    batch_data = np.ones((1, len(s)))\n",
        "    batch_data[0][:] = s\n",
        "    sentence = np.array(batch_data).astype(int)\n",
        "    print(sentence)\n",
        "    output = model(sentence)\n",
        "    outputs = np.argmax(output, axis=2)\n",
        "    labels = list(tag_map.keys())\n",
        "    pred = []\n",
        "    for i in range(len(outputs[0])):\n",
        "        idx = outputs[0][i] \n",
        "        pred_label = labels[idx]\n",
        "        pred.append(pred_label)\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLZCHoiULr3u",
        "scrolled": true
      },
      "source": [
        "# Try the output for the introduction example\n",
        "#sentence = \"Many French citizens are goin to visit Morocco for summer\"\n",
        "#sentence = \"Sharon Floyd flew to Miami last Friday\"\n",
        "\n",
        "# New york times news:\n",
        "sentence = \"Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldn’t necessarily come\"\n",
        "s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
        "predictions = predict(sentence, model, vocab, tag_map)\n",
        "for x,y in zip(sentence.split(' '), predictions):\n",
        "    # if y != 'O':\n",
        "    print(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHYbSnYKnCu6"
      },
      "source": [
        "** Expected Results **\n",
        "\n",
        "```\n",
        "Peter B-per\n",
        "Navarro, I-per\n",
        "White B-org\n",
        "House I-org\n",
        "Sunday B-tim\n",
        "morning I-tim\n",
        "White B-org\n",
        "House I-org\n",
        "coronavirus B-tim\n",
        "fall, B-tim\n",
        "```"
      ]
    }
  ]
}